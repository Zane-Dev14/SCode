{
    "main_ast": {
        "type": "source_file",
        "text": "// src/main.rs\nuse std::process;\nuse clap::Parser;\nuse sys_info::{linux_os_release, os_release, os_type};\nuse llms::gpt::GPT4Model;\n\nmod traits;\nmod llms;\n\n#[derive(Parser)]\n#[command(\nname = \"Clearch\",\nauthor = \"Advaith Narayanan <advaith@glitchy.systems>\",\nabout = \"Search using the command line\",\nafter_help = \"Note: To redirect errors to stdout, use 2>&1.\\n\\\nExample usages:\\n\\\n- Provide a search query: clearch -q \\\"search term\\\"\\n\\\n- Redirect errors: clearch -q \\\"search term\\\" 2>&1\"\n)]\nstruct Gemini {\n    #[arg(short = 'q', long = \"specify\", help = \"Specify the search query to perform\")]\n    search_query: Option<String>,\n}\n\n#[tokio::main]\nasync fn main() {\n    // Retrieve OS information\n    let os_type = os_type().unwrap_or_else(|_| \"Unknown\".to_string());\n    let os_release = os_release().unwrap_or_else(|_| \"Unknown\".to_string());\n    let linux_distro = linux_os_release()\n    .map(|info| info.pretty_name)\n    .unwrap_or(None)\n    .unwrap_or_else(|| \"Unknown\".to_string());\n\n    println!(\n        \"OS: {}  OS REL: {} Linux: {}\",\n        os_type, os_release, linux_distro\n    );\n\n    // Parse command line arguments\n    let search = Gemini::parse();\n\n    // Initialize the GPT-4 model with a hardcoded API key\n    let gpt_model = GPT4Model::new();\n\n    if let Some(query) = search.search_query {\n        println!(\"Searching for: {}\", query);\n\n        // Send the query to GPT-4\n        match gpt_model.req(&query).await {\n            Ok(response) => println!(\"Response: {}\", response),\n            Err(e) => {\n                eprintln!(\"Error: {}\", e);\n                process::exit(1);\n            }\n        }\n    } else {\n        eprintln!(\"Error: Please provide a search query using -q or --specify.\");\n        process::exit(1);\n    }\n}\n"
    },
    "modules_used": [],
    "/app/backend/Clearch/src/traits.rs": {
        "type": "source_file",
        "text": "// src/traits.rs\n\nuse std::error::Error;\n\npub trait LLMRequest {\n    async fn req(&self, query: &str, fine: &str) -> Result<String, Box<dyn Error>>;\n    fn new(api_key: String) -> Self;\n}\n\n"
    },
    "/app/backend/Clearch/src/llms/claude.rs": {
        "type": "source_file",
        "text": "use std::error::Error;\nuse reqwest::Client;\nuse crate::traits::LLMRequest;\n\nstruct ClaudeModel {\n    client: Client,\n    api_key: String,\n}\n\nimpl LLMRequest for ClaudeModel {\n    fn new(api_key: String) -> Self {\n        ClaudeModel{ client: Client::new(), api_key }\n    }\n    async fn req(&self, query: &str, fine: &str) -> Result<String, Box<dyn Error>> {\n        let url = \"https://api.anthropic.com/v1/completions\"; // The Claude API endpoint\n        let response = self.client.post(url)\n            .bearer_auth(&self.api_key)\n            .json(&serde_json::json!({\n                \"model\": fine, // Use the `fine` parameter for the model identifier\n                \"prompt\": query, // Use the `query` parameter for the prompt\n                \"max_tokens\": 100,\n            }))\n            .send()\n            .await?;\n    \n        let text = response.text().await?;\n        Ok(text)\n    }\n    \n}\n"
    },
    "/app/backend/Clearch/src/llms/gemini.rs": {
        "type": "source_file",
        "text": "use reqwest::{header, Client};\nuse serde_json::{json, Value};\nuse std::error::Error;\nuse crate::traits::LLMRequest;\n\n\npub struct GeminiModel {\n    api_key: String,\n    client: Client,\n}\n\nimpl LLMRequest for GeminiModel {\n    fn new(api_key: String) -> Self {\n        GeminiModel { client: Client::new(), api_key }\n    }\n\n    async fn req(&self, query: &str, fine: &str) -> Result<String, Box<dyn Error>> {\n        let map = json!({\n            \"contents\": [\n                {\n                    \"parts\": [\n                        {\n                            \"text\": format!(\"{} {}\", query, fine)\n                        }\n                    ]\n                }\n            ]\n        });\n       \n        let resp = self.client.post(format!(\"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={}\", self.api_key))\n            .header(header::CONTENT_TYPE, \"application/json\")\n            .json(&map)\n            .send()\n            .await?;\n\n        let json: Value = resp.json().await?;\n\n        if let Some(candidate) = json[\"candidates\"].get(0) {\n            if let Some(content) = candidate[\"content\"][\"parts\"].get(0) {\n                if let Some(text) = content[\"text\"].as_str() {\n                    println!(\"{}\", text);\n                    return Ok(text.to_string()); \n                }\n            }\n        }\n        Err(Box::new(std::io::Error::new(std::io::ErrorKind::Other, \"Invalid\")))\n    }\n}\n\n\n\n\n"
    },
    "/app/backend/Clearch/src/llms/gpt.rs": {
        "type": "source_file",
        "text": "use reqwest::Client;\nuse crate::traits::LLMRequest;\n\npub struct GPT4Model {\n    client: Client,\n    api_key: String,\n}\n\nimpl GPT4Model {\n    pub fn new() -> Self {\n        let api_key = \"\".to_string(); // Replace with your actual API key\n        GPT4Model {\n            client: Client::new(),\n            api_key,\n        }\n    }\n\n    pub async fn req(&self, prompt: &str) -> Result<String, reqwest::Error> {\n        let url = \"https://api.openai.com/v1/completions\";\n        let response = self.client.post(url)\n        .bearer_auth(&self.api_key)\n        .json(&serde_json::json!({\n            \"model\": \"gpt-3.5\",\n            \"prompt\": prompt,\n            \"max_tokens\": 100,\n        }))\n        .send()\n        .await?\n        .text()\n        .await?;\n        Ok(response)\n    }\n}\n\n"
    },
    "/app/backend/Clearch/src/llms/mod.rs": {
        "type": "source_file",
        "text": "// src/llms/mod.rs\npub mod claude;\npub mod gemini;\npub mod gpt; // This ensures that gpt.rs is exposed\n"
    }
}